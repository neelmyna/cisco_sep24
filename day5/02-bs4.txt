The `BeautifulSoup` library (part of the `bs4` module) is widely used 
for web scraping in Python. 
It allows you to parse HTML or XML documents and 
extract the desired data.

Here’s a step-by-step guide on how 
to perform web scraping using `BeautifulSoup`:

Step 1: Install Required Libraries

First, install the necessary libraries using `pip`:

```bash
pip install requests
pip install beautifulsoup4
```

Step 2: Send an HTTP Request

To fetch the HTML content of a web page, 
you can use the `requests` library 
to send an HTTP request.

import requests

url = 'https://example.com'
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    print("Successfully retrieved the webpage!")
else:
    print(f"Failed to retrieve the webpage. Status code: {response.status_code}")

Step 3: Parse HTML with `BeautifulSoup`

Once the HTML content is retrieved, you can parse it using `BeautifulSoup`:

from bs4 import BeautifulSoup

# Parse the HTML content
soup = BeautifulSoup(response.content, 'html.parser')

# Print the entire HTML content in a prettified format
print(soup.prettify())

Step 4: Extract Specific Data

To extract specific elements from the page, 
you can use `BeautifulSoup` methods like 
    find(), 
    find_all(), and 
    CSS selectors.

#1. Extracting All Links (`<a>` tags)

You can extract all the hyperlinks (`<a>` tags) from a webpage.

# Find all <a> tags (links)
links = soup.find_all('a')

# Loop through the links and print their href attributes
for link in links:
    href = link.get('href')
    text = link.text.strip()  # Get the text inside the <a> tag
    print(f"Link text: {text}, URL: {href}")

#2. Extracting Elements by Class or ID

You can extract specific elements by class or ID using `find()` or find_all().

# Extract an element with a specific class
element_by_class = soup.find('div', class_='example-class')
print(element_by_class.text)

# Extract an element with a specific ID
element_by_id = soup.find('div', id='example-id')
print(element_by_id.text)
```

#3. Extracting Data Using CSS Selectors

You can also use CSS selectors to extract elements. The `select()` method allows you to use CSS-like queries to find elements.

```python
# Select elements using CSS selectors
selected_elements = soup.select('div.example-class > p')

for element in selected_elements:
    print(element.text)
```

#4. Extracting Table Data

If you're working with tables, you can extract the rows (`<tr>` tags) and individual cells (`<td>` tags).

```python
# Find all rows in a table
table = soup.find('table', class_='example-table')
rows = table.find_all('tr')

for row in rows:
    cells = row.find_all('td')
    for cell in cells:
        print(cell.text)
```

Step 5: Example – Scraping Article Titles

Let's look at a practical example where we extract article titles from a news website.

```python
import requests
from bs4 import BeautifulSoup

# URL of the page to scrape
url = 'https://example-news-website.com'

# Send a GET request to the website
response = requests.get(url)

# Parse the HTML content using BeautifulSoup
soup = BeautifulSoup(response.content, 'html.parser')

# Extract article titles based on the HTML tag and class name
articles = soup.find_all('h2', class_='article-title')

# Loop through each article and print the title
for article in articles:
    title = article.text.strip()
    print(f"Article Title: {title}")
```

Handling Dynamic Content

For dynamic content that is loaded via JavaScript (like on modern websites), 
the `requests` library won’t be enough. 
You may need to use Selenium to control a browser and extract the page content.

Here's a basic example with `Selenium`:

1. Install `selenium`:

```bash
pip install selenium
```

2. Use `Selenium` to interact with the webpage and retrieve dynamically generated content:

```python
from selenium import webdriver
from bs4 import BeautifulSoup

# Set up the WebDriver (for example, for Chrome)
driver = webdriver.Chrome(executable_path='/path/to/chromedriver')

# Open the webpage
driver.get('https://example.com')

# Get the page source (HTML content)
html = driver.page_source

# Parse the HTML using BeautifulSoup
soup = BeautifulSoup(html, 'html.parser')

# Extract specific data (e.g., article titles)
articles = soup.find_all('h2', class_='article-title')

# Print article titles
for article in articles:
    print(article.text)

# Close the browser
driver.quit()
```

Best Practices for Web Scraping

1. Respect the Website’s `robots.txt`: Always check the `robots.txt` file of the website to see if scraping is allowed.
2. Use Headers: Websites often block requests without a user-agent header, so you can add it to mimic a browser request.
   
   ```python
   headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
   response = requests.get(url, headers=headers)
   ```

3. Rate Limiting: Avoid overwhelming the server by making requests too frequently. Use time delays between requests using the `time.sleep()` function.

4. Error Handling: Use proper error handling for failed requests or timeouts.
   
   ```python
   try:
       response = requests.get(url)
       response.raise_for_status()  # Raises an error for bad responses
   except requests.exceptions.HTTPError as err:
       print(f"HTTP error: {err}")
   except Exception as err:
       print(f"Other error: {err}")
   ```

Conclusion

`BeautifulSoup` combined with `requests` is a powerful and simple way 
to scrape static websites. For dynamic content, 
you may need to use `Selenium`. 
Always ensure you follow ethical guidelines and 
website policies when scraping content.